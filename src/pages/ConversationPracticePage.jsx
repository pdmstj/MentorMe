import React, { useRef, useEffect, useState } from 'react';
import { useNavigate, useLocation } from 'react-router-dom';
import '../pages/ConversationPracticePage.css';  // ë‘ë²ˆì§¸ ìŠ¤íƒ€ì¼ css
import frame34 from "../image/Frame 34.svg";
import ai_men from "../image/aimento.png";
import logoImg from "../image/mentorme_logo.png";
import { fetchQuestionsByCategory } from '../services/questionService';

const categoryMap = {
  "ê¸°íš": "management",
  "ê²½ì˜": "management",
  "ë§ˆì¼€íŒ…": "management",
  "ê¸°ìˆ ": "develop",
  "ê°œë°œ": "develop",
  "ë””ìì¸": "design",
  "ë¯¸ë””ì–´": "design",
  "ì½˜í…ì¸ ": "design",
  "ê±´ë„ˆë›°ê¸°": "tenacity"
};

const SelfInterviewPracticeStyled = () => {
  const navigate = useNavigate();
  const location = useLocation();

  const rawCategory = (location.state?.interest || "").trim();
  const category = categoryMap[rawCategory] ?? "design";

  const videoRef = useRef(null);
  const mediaRecorderRef = useRef(null);
  const chunksRef = useRef([]);
  const [mediaStream, setMediaStream] = useState(null);
  const [recording, setRecording] = useState(false);
  const [recordedBlob, setRecordedBlob] = useState(null);
  const [questions, setQuestions] = useState([]);
  const [questionIndex, setQuestionIndex] = useState(0);
  const [recordingTime, setRecordingTime] = useState(0);
  const [loading, setLoading] = useState(false);

  // ì§ˆë¬¸ ë¶ˆëŸ¬ì˜¤ê¸°
  useEffect(() => {
    const loadQuestions = async () => {
      try {
        const qs = await fetchQuestionsByCategory(category);
        if (!qs || qs.length === 0) {
          alert("í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.");
        }
        setQuestions(qs);
      } catch (err) {
        console.error("ì§ˆë¬¸ ë¡œë”© ì‹¤íŒ¨:", err);
      }
    };
    loadQuestions();
  }, [category]);

  // 30ì´ˆë§ˆë‹¤ ì§ˆë¬¸ ë³€ê²½ ë° TTS ì‹¤í–‰
  useEffect(() => {
    if (questions.length === 0) return;

    // ì§ˆë¬¸ì„ ìŒì„±ìœ¼ë¡œ ì½ì–´ì£¼ëŠ” í•¨ìˆ˜
    const speakQuestion = (text) => {
      if (!window.speechSynthesis) return;
      window.speechSynthesis.cancel();  // ê¸°ì¡´ ì½ê¸° ì·¨ì†Œ

      const utterance = new SpeechSynthesisUtterance(text);
      utterance.lang = 'ko-KR'; // í•œêµ­ì–´ ì„¤ì •
      window.speechSynthesis.speak(utterance);
    };

    // ì²« ì§ˆë¬¸ ì½ê¸°
    speakQuestion(questions[questionIndex]);

    const timer = setInterval(() => {
      setQuestionIndex((prev) => {
        const nextIndex = (prev + 1) % questions.length;
        speakQuestion(questions[nextIndex]);
        return nextIndex;
      });
    }, 30000);

    return () => {
      clearInterval(timer);
      window.speechSynthesis.cancel();
    };
  }, [questions]);

  // ì›¹ìº  ë° ë…¹í™” ì‹œì‘
  useEffect(() => {
    const startCameraAndRecording = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });
        setMediaStream(stream);
        if (videoRef.current) videoRef.current.srcObject = stream;

        const mediaRecorder = new MediaRecorder(stream);
        mediaRecorderRef.current = mediaRecorder;
        chunksRef.current = [];

        mediaRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) chunksRef.current.push(e.data);
        };

        mediaRecorder.onstop = async () => {
          setLoading(true);

          const blob = new Blob(chunksRef.current, { type: 'video/webm' });
          setRecordedBlob(blob);
          const videoUrl = URL.createObjectURL(blob);

          // ì˜¤ë””ì˜¤ ì¶”ì¶œ ëŒ€ì‹  ê·¸ëŒ€ë¡œ Blob ì „ì†¡ (ì›ë˜ ë¡œì§ ìœ ì§€)
          const audioBlob = blob;

          const file = new File([audioBlob], 'audio.webm', { type: 'audio/webm' });
          const formData = new FormData();
          formData.append('file', file);

          try {
            const response = await fetch('http://localhost:8000/stt/', {
              method: 'POST',
              body: formData,
            });

            const data = await response.json();
            console.log("STT ê²°ê³¼: ", data.text);

            navigate('/feedback', {
              state: {
                videoUrl,
                sttText: data.text,
                type: 'self',
              }
            });
          } catch (error) {
            console.error('STT ì„œë²„ ì—ëŸ¬:', error);
            alert('ìŒì„± ì¸ì‹ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
            setLoading(false);
          }

          stream.getTracks().forEach((track) => track.stop());
        };

        mediaRecorder.start();
        setRecording(true);
      } catch (err) {
        console.error('ì›¹ìº  ì ‘ê·¼ ì˜¤ë¥˜:', err);
      }
    };

    startCameraAndRecording();
  }, []);

  // ë…¹í™” ì‹œê°„ íƒ€ì´ë¨¸
  useEffect(() => {
    if (!recording) return;
    const timer = setInterval(() => {
      setRecordingTime((prev) => prev + 1);
    }, 1000);
    return () => clearInterval(timer);
  }, [recording]);

  const formatTime = (seconds) => {
    const m = String(Math.floor(seconds / 60)).padStart(2, '0');
    const s = String(seconds % 60).padStart(2, '0');
    return `${m}:${s}`;
  };

  const handleFeedbackClick = () => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      setLoading(true);
      mediaRecorderRef.current.stop();
      setRecording(false);
    } else if (recordedBlob) {
      const videoUrl = URL.createObjectURL(recordedBlob);
      navigate('/feedback', {
        state: { videoUrl, sttText: "", type: 'self' }
      });
    } else {
      alert('ë…¹í™”ëœ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤.');
    }
  };

  return (
    <>
      {loading && (
        <div className="loading-overlay">
          <div className="spinner" />
          <p>ğŸŒ€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...</p>
          ë¶„ì„ ì‹œê°„ì´ ë§ì´ ì§€ì—°ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤ !
        </div>
      )}

      {!loading && (
        <>
          <div className="header-container">
            <div className="title-box">
              <div className="title-logo">
                <img src={logoImg} alt="ë¡œê³ " className="logo-img" />
              </div>
              <span className="title-text">ì…€í”„ ì‹¤ì „ ë©´ì ‘</span>
            </div>
            <button className="exit-button" onClick={() => navigate('/')}>ë‚˜ê°€ê¸°</button>
          </div>
          <hr className="hrline" />
          <div className="interview-container">
            <div className="question-container">
              <img className="img" alt="Frame" src={frame34} />
              <div className="question-section">
                <span className="question-tag">ğŸ§  ì§ˆë¬¸</span>
                <p className="question-text">
                  {questions.length > 0 ? questions[questionIndex] : "ì§ˆë¬¸ì„ ë¶ˆëŸ¬ì˜¤ê³  ìˆìŠµë‹ˆë‹¤..."}
                </p>
              </div>
            </div>

            <div className="interview-video-section">
              <div className="ai-interviewer">
                <p>AI ë©´ì ‘ê´€ í™”ë©´</p>
                <img src={ai_men} alt="AI ë©´ì ‘ê´€" className="face-image" />
              </div>

              <div className="user-camera">
                <p>ë‚˜ì˜ í™”ë©´</p>
                <video ref={videoRef} autoPlay muted playsInline className="video-preview" />

                {recording && (
                  <div className="recording-bar">
                    <div className="recording-indicator">ğŸ”´ ë…¹í™”ì¤‘ {formatTime(recordingTime)}</div>
                    <div className="waveform">
                      <div className="bar"></div>
                      <div className="bar"></div>
                      <div className="bar"></div>
                      <div className="bar"></div>
                      <div className="bar"></div>
                    </div>
                    <button className="feedback-button" onClick={handleFeedbackClick}>í”¼ë“œë°± ë³´ëŸ¬ê°€ê¸°</button>
                  </div>
                )}
              </div>
            </div>
          </div>
        </>
      )}
    </>
  );
};

export default SelfInterviewPracticeStyled;
